{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install bnunicodenormalizer\n",
    "# !pip install jiwer\n",
    "# !pip install nltk\n",
    "# !pip install librosa\n",
    "# !pip install pandas\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import string\n",
    "import nltk\n",
    "import math\n",
    "import itertools\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "from torch import nn, optim\n",
    "from joblib import Parallel, delayed\n",
    "from jiwer import wer, cer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bnunicodenormalizer import Normalizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000005f3362c</td>\n",
       "      <td>ও বলেছে আপনার ঠিকানা!</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001dddd002</td>\n",
       "      <td>কোন মহান রাষ্ট্রের নাগরিক হতে চাও?</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001e0bc131</td>\n",
       "      <td>আমি তোমার কষ্টটা বুঝছি, কিন্তু এটা সঠিক পথ না।</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000024b3d810</td>\n",
       "      <td>নাচ শেষ হওয়ার পর সকলে শরীর ধুয়ে একসঙ্গে ভোজন...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000028220ab3</td>\n",
       "      <td>হুমম, ওহ হেই, দেখো।</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                           sentence  split\n",
       "0  000005f3362c                              ও বলেছে আপনার ঠিকানা!  train\n",
       "1  00001dddd002                 কোন মহান রাষ্ট্রের নাগরিক হতে চাও?  train\n",
       "2  00001e0bc131     আমি তোমার কষ্টটা বুঝছি, কিন্তু এটা সঠিক পথ না।  train\n",
       "3  000024b3d810  নাচ শেষ হওয়ার পর সকলে শরীর ধুয়ে একসঙ্গে ভোজন...  train\n",
       "4  000028220ab3                                হুমম, ওহ হেই, দেখো।  train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and examine data\n",
    "train_df = pd.read_csv('./input/bengaliai-speech/train.csv')\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Data selection for prototyping (delete/comment out for full training)\n",
    "NUM_EXAMPLES = 50000\n",
    "group = 1\n",
    "if group < 5:\n",
    "    train_df = train_df.iloc[NUM_EXAMPLES*(group-1):NUM_EXAMPLES*group, :]\n",
    "else:\n",
    "    train_df = train_df.iloc[NUM_EXAMPLES*(group-1):, :]\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:17, 2797.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 unique characters\n",
      "Maximum sentence length = 21 words\n",
      "The vocabulary includes 55710 words\n"
     ]
    }
   ],
   "source": [
    "bnorm = Normalizer()\n",
    "def process_text(sentence):    \n",
    "    # Remove special characters\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Normalize sentences with bnunicodenormalizer\n",
    "    tokens = word_tokenize(sentence)\n",
    "    norm_tokens = [bnorm(token)['normalized'] for token in tokens]\n",
    "    norm_sentence = \" \".join([word for word in norm_tokens if word is not None])\n",
    "    \n",
    "    # Get unique characters\n",
    "    unique_sentence_chars = list(set([*norm_sentence]))\n",
    "    \n",
    "    return norm_sentence, unique_sentence_chars\n",
    "\n",
    "# Get all unique characters in data set and normalize sentences\n",
    "norm_sentences, unique_chars_per_sentence = zip(*Parallel(n_jobs=-1)(delayed(process_text)\n",
    "                                                                     (train_df.at[index, 'sentence']) \n",
    "                                                                     for index, row in tqdm(train_df.iterrows())))\n",
    "\n",
    "all_chars = list(itertools.chain.from_iterable(unique_chars_per_sentence))\n",
    "unique_chars = list(set(all_chars))\n",
    "unique_chars.append('_') # add label space to character labels\n",
    "print(str(len(unique_chars)) + \" unique characters\")\n",
    "\n",
    "# Get maximum sentence length\n",
    "max_sentence_length = max([len(sentence.split(' ')) for sentence in norm_sentences])\n",
    "print('Maximum sentence length = ' + str(max_sentence_length) + ' words')\n",
    "\n",
    "# Get vocabulary size\n",
    "all_words = list(itertools.chain(*[sentence.split(' ') for sentence in norm_sentences]))\n",
    "unique_words = list(set(all_words))\n",
    "vocabulary_size = len(Counter(all_words).keys())\n",
    "print('The vocabulary includes ' + str(vocabulary_size) + ' words')\n",
    "\n",
    "# Save normalized dataset\n",
    "train_df['sentence'] = norm_sentences\n",
    "train_df.to_csv('./working/norm_train.csv', index=False)\n",
    "\n",
    "# # Save list of unique characters\n",
    "# with open('./working/unique_chars.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     for char in unique_chars:\n",
    "#         f.write(\"%s\\n\" % char)\n",
    "        \n",
    "# Clean up\n",
    "del all_chars\n",
    "del all_words\n",
    "del unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define torch Dataset for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dawson\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchaudio\\functional\\functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.io import read_image\n",
    "SAMPLE_RATE=16000\n",
    "\n",
    "# Frequency and time masking only for training\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=128, power=2.0),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=0),\n",
    "    nn.Unflatten(dim=0, unflattened_size=(1, -1, 128)), # TimeMasking expects a batch dimension\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=0)\n",
    ")\n",
    "# Just get spectrogram for validation\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=128)\n",
    "\n",
    "class BengaliAIDataset(Dataset):\n",
    "    def __init__(self, utterance_file, audio_dir, data_type='training', \n",
    "                 transform=None, target_transform=None):\n",
    "        self.audio_dir = audio_dir\n",
    "        if data_type in ['training', 'validation']:\n",
    "            self.sentence_labels = pd.read_csv(utterance_file)\n",
    "        else:\n",
    "            self.sentence_labels = os.listdir(self.audio_dir)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data_type = data_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_type in ['training', 'validation']:\n",
    "            audio_path = os.path.join(self.audio_dir,\n",
    "                                      self.sentence_labels.iloc[idx, 0] + '.mp3')\n",
    "            waveform, sr = librosa.load(audio_path, sr=32000)\n",
    "            label = self.sentence_labels.iloc[idx, 1]\n",
    "            if self.transform:\n",
    "                waveform = self.transform(waveform)\n",
    "            if self.target_transform:\n",
    "                label = self.target_transform(label)\n",
    "            return waveform, label\n",
    "        else:\n",
    "            file_list = os.listdir(self.audio_dir)\n",
    "            audio_path = os.path.join(self.audio_dir,\n",
    "                                      file_list[idx])\n",
    "            waveform, sr = librosa.load(audio_path, sr=32000)\n",
    "            if self.transform:\n",
    "                waveform = self.transform(waveform)\n",
    "            return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data processing before each batch\n",
    "def data_processing(data, data_type='training'):\n",
    "    labeled_sentences = []\n",
    "    spectrograms = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    if data_type in ['training', 'validation']:\n",
    "        for (waveform, sentence) in data:\n",
    "            # Resample waveform and convert to Tensor\n",
    "            waveform = librosa.resample(waveform, orig_sr=32000, target_sr=16000)\n",
    "            waveform = torch.Tensor(waveform)\n",
    "\n",
    "            # Audio transform during training and inference\n",
    "            if data_type == 'training':\n",
    "                spec = train_audio_transforms(waveform).squeeze().transpose(0, 1)\n",
    "            else:\n",
    "                spec = valid_audio_transforms(waveform).squeeze().transpose(0, 1)\n",
    "            spec = librosa.power_to_db(spec, ref=np.max)\n",
    "            spectrograms.append(torch.Tensor(spec))\n",
    "\n",
    "            # Map characters to integer labels and get input lengths\n",
    "            sentence_labels = []\n",
    "            for char in sentence:\n",
    "                sentence_labels.append(unique_chars.index(char))\n",
    "            label = torch.Tensor(sentence_labels)\n",
    "            labeled_sentences.append(label)\n",
    "\n",
    "            input_lengths.append(spec.shape[0]//2)\n",
    "            label_lengths.append(len(sentence))\n",
    "\n",
    "        # Pad sequences\n",
    "        spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True, padding_value=-80).unsqueeze(1).transpose(2, 3)\n",
    "        labels = nn.utils.rnn.pad_sequence(labeled_sentences, batch_first=True, padding_value=len(unique_chars)+1)\n",
    "    \n",
    "        return spectrograms, labels, input_lengths, label_lengths\n",
    "    \n",
    "    else: # For evaluation\n",
    "        for waveform in data:\n",
    "            # Resample waveform and convert to Tensor\n",
    "            waveform = librosa.resample(waveform, orig_sr=32000, target_sr=16000)\n",
    "            waveform = torch.Tensor(waveform)\n",
    "\n",
    "            spec = valid_audio_transforms(waveform).squeeze().transpose(0, 1)\n",
    "            spec = librosa.power_to_db(spec, ref=np.max)\n",
    "            spectrograms.append(torch.Tensor(spec))\n",
    "\n",
    "            input_lengths.append(spec.shape[0])\n",
    "\n",
    "        # Pad sequences\n",
    "        spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True, padding_value=-80).unsqueeze(1).transpose(2, 3)\n",
    "\n",
    "        return spectrograms, input_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model based on post at \"https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/\"\"\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "    \n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "    \n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
    "\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic greedy decoder (beam-search later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./working/unique_chars.txt\", \"r\", encoding=\"utf-8\")\n",
    "unique_chars = [label.replace('\\n', '') for label in f.readlines()]\n",
    "\n",
    "def GreedyDecoder(output, blank_label=len(unique_chars)-1, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(unique_chars[index])\n",
    "        decodes.append(decode)\n",
    "    return decodes\n",
    "\n",
    "def norm_decoded_results(decoded_results):\n",
    "    bnorm = Normalizer()\n",
    "    evaluated_sentences = []\n",
    "    i=0\n",
    "    for utterance in decoded_results:\n",
    "        try:\n",
    "            # Normalize sentences with bnunicodenormalizer\n",
    "            utterance = \"\".join(utterance)\n",
    "            norm_tokens = [bnorm(word)['normalized'] for word in utterance.split()]\n",
    "            norm_sentence = \"\".join([word for word in norm_tokens if word is not None])\n",
    "        except:\n",
    "            norm_sentence = ','\n",
    "\n",
    "        # Combine decoded results into sentence\n",
    "        evaluated_sentences.append(norm_sentence)\n",
    "    \n",
    "    return evaluated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KenLM lexicon file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 195645.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29605 lines saved out of 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ken_vocab = open(\"./working/bn.sp.vocab\", \"r\", encoding='utf-8')\n",
    "lines = ken_vocab.readlines()\n",
    "\n",
    "line_count = 0\n",
    "with open('./working/lexicon.txt', 'w', encoding='utf-8') as file:\n",
    "    for line in tqdm(lines):\n",
    "        # Formatting from KenLM vocab file\n",
    "        line = line.split('\\t')\n",
    "        if '▁' in line[0]:\n",
    "            line = line[0].replace('▁', '')\n",
    "        else:\n",
    "            line = line[0]\n",
    "\n",
    "        char_list = [char for char in line]\n",
    "        # Do not process lines if they contain invalid characters\n",
    "        if any(map(lambda char: char not in unique_chars, char_list)):\n",
    "            continue\n",
    "        elif line == '':\n",
    "            continue\n",
    "        else:\n",
    "            processed_string = line + \" \" + \" \".join(line) + \" _\"\n",
    "            file.write(processed_string + '\\n')\n",
    "\n",
    "            line_count += 1\n",
    "print(str(line_count) + ' lines saved out of ' + str(len(lines)))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search decoder with KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BeamSearchDecoder(BEAM_SIZE, LM_WEIGHT):\n",
    "    decoder = ctc_decoder(lm=\"./working/bn.arpa.bin\",\n",
    "                          lexicon=\"./working/lexicon.txt\",\n",
    "                          tokens=\"./working/unique_chars.txt\",\n",
    "                          beam_size=BEAM_SIZE,\n",
    "                          lm_weight=LM_WEIGHT,\n",
    "                          sil_token=\" \",\n",
    "                          blank_token=\"_\")\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, scaler):\n",
    "    model.train()\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    data_len = len(train_loader.dataset)\n",
    "    running_loss = 0\n",
    "    for batch_idx, _data in tqdm(enumerate(train_loader)):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data \n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device_type=str(device), dtype=torch.float16):\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "            \n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        iter_meter.step()\n",
    "        if batch_idx % 250 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                100. * batch_idx / len(train_loader), running_loss/250))\n",
    "            running_loss = 0\n",
    "                \n",
    "def validate(model, device, val_loader, criterion, epoch, iter_meter, decode_mode='greedy'):\n",
    "    print('\\nvalidating…')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, _data in enumerate(val_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(val_loader)\n",
    "            \n",
    "            if decode_mode == 'greedy':\n",
    "                # Compute WER and CER\n",
    "                decoded_preds = GreedyDecoder(output.transpose(0, 1))\n",
    "                decoded_preds = norm_decoded_results(decoded_preds)\n",
    "            else:\n",
    "                decoded_preds = BeamSearchDecoder(1000, 3.23)\n",
    "            \n",
    "            decoded_labels = []\n",
    "            for row in range(labels.shape[0]):\n",
    "                # Remove zero padding from left side of labels\n",
    "                unpadded_label = labels[row, labels[row,:]!=len(unique_chars)+1] # Pad character is set to len(unique_chars+1) in data_processing\n",
    "                decoded_labels.append(''.join(unique_chars[int(token.item())] for token in unpadded_label))\n",
    "\n",
    "            for i in range(len(decoded_preds)):\n",
    "                test_wer.append(wer(decoded_labels[i], decoded_preds[i]))\n",
    "                test_cer.append(cer(decoded_labels[i], decoded_preds[i]))\n",
    "                \n",
    "        if len(test_wer) > 0:\n",
    "            avg_wer = sum(test_wer)/len(test_wer)\n",
    "            avg_cer = sum(test_cer)/len(test_cer)\n",
    "        else:\n",
    "            avg_wer = 100\n",
    "            avg_cer = 100\n",
    "        \n",
    "        print('Train Epoch: {} \\t Validation loss: {:.6f}'.format(\n",
    "                epoch, test_loss))\n",
    "        print('Train Epoch: {} \\t Validation WER: {:.6f}'.format(\n",
    "                epoch, avg_wer))\n",
    "        print('Train Epoch: {} \\t Validation CER: {:.6f}'.format(\n",
    "                epoch, avg_cer))\n",
    "        \n",
    "        return test_loss, wer, cer\n",
    "\n",
    "def evaluate(model, device, eval_loader, iter_meter, decode_mode='greedy'):\n",
    "    print('\\nevaluating…')\n",
    "    model.eval()\n",
    "    decoded_results = []\n",
    "    with torch.no_grad():\n",
    "        for I, _data in enumerate(eval_loader):\n",
    "            spectrograms, input_lengths = _data \n",
    "            spectrograms = spectrograms.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "            \n",
    "            if decode_mode == 'greedy':\n",
    "                decoded_preds = GreedyDecoder(output.transpose(0, 1))\n",
    "                decoded_preds = norm_decoded_results(decoded_preds)\n",
    "            else:\n",
    "                decoded_preds = BeamSearchDecoder(1000, 3.23)\n",
    "    \n",
    "            decoded_results.append(decoded_preds)\n",
    "        \n",
    "    return decoded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnorm = Normalizer()\n",
    "\n",
    "# # Instantiate training and validation dataset\n",
    "# train_audio_directory = './input/bengaliai-speech/train_mp3s/'\n",
    "# train_label_path = './working/norm_train.csv'\n",
    "# train_dataset, valid_dataset = torch.utils.data.random_split(BengaliAIDataset(train_label_path, train_audio_directory), \n",
    "#                                                              [round(len(BengaliAIDataset(train_label_path, train_audio_directory))*0.99),\n",
    "#                                                               round(len(BengaliAIDataset(train_label_path, train_audio_directory))*0.01)])\n",
    "\n",
    "# hparams = {\n",
    "#     \"n_cnn_layers\": 3,\n",
    "#     \"n_rnn_layers\": 3,\n",
    "#     \"rnn_dim\": 512,\n",
    "#     \"n_class\": len(unique_chars),\n",
    "#     \"n_feats\": 128,\n",
    "#     \"stride\": 2,\n",
    "#     \"dropout\": 0.1,\n",
    "#     \"learning_rate\": 1e-1,\n",
    "#     \"batch_size\": 1,\n",
    "#     \"epochs\": 1\n",
    "# }\n",
    "\n",
    "\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# torch.manual_seed(7)\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# if not os.path.isdir(\"./data\"):\n",
    "#     os.makedirs(\"./data\")\n",
    "\n",
    "# kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "# val_loader = DataLoader(dataset=valid_dataset,\n",
    "#                         batch_size=hparams['batch_size'],\n",
    "#                         shuffle=False,\n",
    "#                         collate_fn=lambda x: data_processing(x, 'validation'),\n",
    "#                         **kwargs)\n",
    "\n",
    "# criterion = nn.CTCLoss(blank=len(unique_chars)-1, zero_infinity=True).to(device)\n",
    "\n",
    "# iter_meter = IterMeter()\n",
    "# test_loss, test_wer, test_cer = validate(trained_model, device, val_loader, criterion, 1, iter_meter, decode_mode='beam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validation(learning_rate=5e-4, batch_size=4, epochs=1):\n",
    "    \n",
    "    torch.manual_seed(7) # For reproducibility\n",
    "    \n",
    "    # Instantiate training and validation dataset\n",
    "    train_audio_directory = './input/bengaliai-speech/train_mp3s/'\n",
    "    train_label_path = './working/norm_train.csv'\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(BengaliAIDataset(train_label_path, train_audio_directory), \n",
    "                                                                 [round(len(BengaliAIDataset(train_label_path, train_audio_directory))*0.999),\n",
    "                                                                  round(len(BengaliAIDataset(train_label_path, train_audio_directory))*0.001)])\n",
    "\n",
    "    hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 3,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": len(unique_chars),\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print('Training device: ' + str(device))\n",
    "\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "\n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=hparams['batch_size'],\n",
    "                              shuffle=True,  # Change to True for full training runs\n",
    "                              collate_fn=lambda x: data_processing(x, 'training'),\n",
    "                              **kwargs)\n",
    "    \n",
    "    val_loader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=hparams['batch_size'],\n",
    "                            shuffle=False,\n",
    "                            collate_fn=lambda x: data_processing(x, 'validation'),\n",
    "                            **kwargs)\n",
    "\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=len(unique_chars)-1, zero_infinity=True).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                              steps_per_epoch=int(len(train_loader)),\n",
    "                                              epochs=hparams['epochs'],\n",
    "                                              anneal_strategy='cos',\n",
    "                                              div_factor=10,\n",
    "                                              final_div_factor=100,\n",
    "                                              pct_start=0.1)\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    prev_loss = 10\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, scaler)\n",
    "        val_loss, val_wer, val_cer = validate(model, device, val_loader, criterion, epoch, iter_meter)\n",
    "        \n",
    "        if val_loss < prev_loss:\n",
    "            print('Saving checkpoint...')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'val loss': val_loss,\n",
    "                'val_wer': val_wer,\n",
    "                'val_cer': val_cer\n",
    "            }, './training_checkpoint.tar')\n",
    "            \n",
    "            prev_loss = val_loss\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_from_checkpoint(learning_rate=5e-4, batch_size=4, epochs=1):\n",
    "    torch.manual_seed(7) # For reproducibility\n",
    "    \n",
    "    # Instantiate training and validation dataset\n",
    "    train_audio_directory = './input/bengaliai-speech/train_mp3s/'\n",
    "    train_label_path = './working/norm_train.csv'\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(BengaliAIDataset(train_label_path, train_audio_directory), \n",
    "                                                                 [round(len(BengaliAIDataset(train_label_path, train_audio_directory))*0.999),\n",
    "                                                                  round(len(BengaliAIDataset(train_label_path, train_audio_directory))*0.001)])\n",
    "    \n",
    "    hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 3,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": len(unique_chars),\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print('Training device: ' + str(device))\n",
    "\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "        \n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=hparams['batch_size'],\n",
    "                              shuffle=True,  # Change to True for full training runs\n",
    "                              collate_fn=lambda x: data_processing(x, 'training'),\n",
    "                              **kwargs)\n",
    "    \n",
    "    val_loader = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=hparams['batch_size'],\n",
    "                            shuffle=False,\n",
    "                            collate_fn=lambda x: data_processing(x, 'validation'),\n",
    "                            **kwargs)\n",
    "    \n",
    "    checkpoint = torch.load('./training_checkpoint.tar')\n",
    "\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    criterion = nn.CTCLoss(blank=len(unique_chars)-1, zero_infinity=True).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                              steps_per_epoch=int(len(train_loader)),\n",
    "                                              epochs=hparams['epochs'],\n",
    "                                              anneal_strategy='cos',\n",
    "                                              div_factor=10,\n",
    "                                              final_div_factor=100,\n",
    "                                              pct_start=0.1)\n",
    "    \n",
    "    last_epoch = checkpoint['epoch']\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    prev_loss = checkpoint['val loss']\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch+last_epoch, iter_meter, scaler)\n",
    "        val_loss, val_wer, val_cer = validate(model, device, val_loader, criterion, epoch+last_epoch, iter_meter)\n",
    "        \n",
    "        if val_loss < prev_loss:\n",
    "            print('Saving checkpoint...')\n",
    "            torch.save({\n",
    "                'epoch': epoch+last_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'val loss': val_loss,\n",
    "                'val_wer': val_wer,\n",
    "                'val_cer': val_cer\n",
    "            }, './training_checkpoint.tar')\n",
    "        \n",
    "            prev_loss = val_loss\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n",
      "Num Model Parameters 14271299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\Dawson\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "1it [00:02,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/49950 (0%)]\tLoss: 0.050877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [03:03,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [8000/49950 (16%)]\tLoss: 3.852864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [05:41,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [16000/49950 (32%)]\tLoss: 3.419862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [08:42,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [24000/49950 (48%)]\tLoss: 3.288844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [11:32,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32000/49950 (64%)]\tLoss: 2.989964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [14:34,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [40000/49950 (80%)]\tLoss: 2.351088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [17:40,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [48000/49950 (96%)]\tLoss: 2.081092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [18:18,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating…\n",
      "Train Epoch: 1 \t Validation loss: 1.824402\n",
      "Train Epoch: 1 \t Validation WER: 1.000000\n",
      "Train Epoch: 1 \t Validation CER: 0.609132\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/49950 (0%)]\tLoss: 0.006134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [02:44,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [8000/49950 (16%)]\tLoss: 1.916212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [05:09,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [16000/49950 (32%)]\tLoss: 1.847106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [08:07,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [24000/49950 (48%)]\tLoss: 1.798712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [10:38,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [32000/49950 (64%)]\tLoss: 1.745762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [13:03,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [40000/49950 (80%)]\tLoss: 1.716117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [15:42,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [48000/49950 (96%)]\tLoss: 1.685556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [16:27,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating…\n",
      "Train Epoch: 2 \t Validation loss: 1.499460\n",
      "Train Epoch: 2 \t Validation WER: 1.000000\n",
      "Train Epoch: 2 \t Validation CER: 0.521305\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/49950 (0%)]\tLoss: 0.006649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [03:03,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [8000/49950 (16%)]\tLoss: 1.586286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [06:01,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [16000/49950 (32%)]\tLoss: 1.570807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [08:53,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [24000/49950 (48%)]\tLoss: 1.576487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [11:41,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [32000/49950 (64%)]\tLoss: 1.574965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [14:38,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [40000/49950 (80%)]\tLoss: 1.582421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [17:39,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [48000/49950 (96%)]\tLoss: 1.527894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [18:23,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating…\n",
      "Train Epoch: 3 \t Validation loss: 1.370430\n",
      "Train Epoch: 3 \t Validation WER: 1.000000\n",
      "Train Epoch: 3 \t Validation CER: 0.496907\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/49950 (0%)]\tLoss: 0.005603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [02:36,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [8000/49950 (16%)]\tLoss: 1.463271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [04:58,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [16000/49950 (32%)]\tLoss: 1.445374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [07:30,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [24000/49950 (48%)]\tLoss: 1.459617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [09:55,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [32000/49950 (64%)]\tLoss: 1.435465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [12:47,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [40000/49950 (80%)]\tLoss: 1.419493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [15:39,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [48000/49950 (96%)]\tLoss: 1.433379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [16:15,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating…\n",
      "Train Epoch: 4 \t Validation loss: 1.301237\n",
      "Train Epoch: 4 \t Validation WER: 1.000000\n",
      "Train Epoch: 4 \t Validation CER: 0.482890\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/49950 (0%)]\tLoss: 0.005165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [02:42,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [8000/49950 (16%)]\tLoss: 1.318625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [05:22,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [16000/49950 (32%)]\tLoss: 1.351418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [07:55,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [24000/49950 (48%)]\tLoss: 1.344754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [10:40,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [32000/49950 (64%)]\tLoss: 1.357612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [13:11,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [40000/49950 (80%)]\tLoss: 1.333663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [15:55,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [48000/49950 (96%)]\tLoss: 1.330622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [16:31,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating…\n",
      "Train Epoch: 5 \t Validation loss: 1.256645\n",
      "Train Epoch: 5 \t Validation WER: 1.000000\n",
      "Train Epoch: 5 \t Validation CER: 0.464706\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/49950 (0%)]\tLoss: 0.004725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [02:49,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [8000/49950 (16%)]\tLoss: 1.228573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [05:16,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [16000/49950 (32%)]\tLoss: 1.272220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [07:58,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [24000/49950 (48%)]\tLoss: 1.222336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [10:37,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [32000/49950 (64%)]\tLoss: 1.250995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [13:19,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [40000/49950 (80%)]\tLoss: 1.245985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [16:05,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [48000/49950 (96%)]\tLoss: 1.248095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [16:40,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating…\n",
      "Train Epoch: 6 \t Validation loss: 1.220248\n",
      "Train Epoch: 6 \t Validation WER: 1.000000\n",
      "Train Epoch: 6 \t Validation CER: 0.456940\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/49950 (0%)]\tLoss: 0.003965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [02:42,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [8000/49950 (16%)]\tLoss: 1.164817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [05:11,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [16000/49950 (32%)]\tLoss: 1.163111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [07:38,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [24000/49950 (48%)]\tLoss: 1.170771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [10:21,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [32000/49950 (64%)]\tLoss: 1.162407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [13:06,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [40000/49950 (80%)]\tLoss: 1.162725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [15:42,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [48000/49950 (96%)]\tLoss: 1.153265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [16:27,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating…\n",
      "Train Epoch: 7 \t Validation loss: 1.217319\n",
      "Train Epoch: 7 \t Validation WER: 1.000000\n",
      "Train Epoch: 7 \t Validation CER: 0.450756\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/49950 (0%)]\tLoss: 0.004625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [02:50,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [8000/49950 (16%)]\tLoss: 1.097505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [05:26,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [16000/49950 (32%)]\tLoss: 1.088695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [08:09,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [24000/49950 (48%)]\tLoss: 1.091081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [10:44,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [32000/49950 (64%)]\tLoss: 1.091804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [13:20,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [40000/49950 (80%)]\tLoss: 1.102768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [15:48,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [48000/49950 (96%)]\tLoss: 1.099754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [16:26,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating…\n",
      "Train Epoch: 8 \t Validation loss: 1.183048\n",
      "Train Epoch: 8 \t Validation WER: 1.000000\n",
      "Train Epoch: 8 \t Validation CER: 0.435949\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/49950 (0%)]\tLoss: 0.004806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [02:33,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [8000/49950 (16%)]\tLoss: 1.051110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [04:55,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [16000/49950 (32%)]\tLoss: 1.045682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [07:39,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [24000/49950 (48%)]\tLoss: 1.047423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [10:29,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [32000/49950 (64%)]\tLoss: 1.059599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [13:02,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [40000/49950 (80%)]\tLoss: 1.054683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [15:41,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [48000/49950 (96%)]\tLoss: 1.045741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [16:18,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating…\n",
      "Train Epoch: 9 \t Validation loss: 1.182050\n",
      "Train Epoch: 9 \t Validation WER: 1.000000\n",
      "Train Epoch: 9 \t Validation CER: 0.433763\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/49950 (0%)]\tLoss: 0.004241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [02:37,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [8000/49950 (16%)]\tLoss: 1.041600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [05:30,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [16000/49950 (32%)]\tLoss: 1.021434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [08:15,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [24000/49950 (48%)]\tLoss: 1.026418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [10:45,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [32000/49950 (64%)]\tLoss: 1.025895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [13:15,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [40000/49950 (80%)]\tLoss: 1.020143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [16:03,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [48000/49950 (96%)]\tLoss: 1.030403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [16:41,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating…\n",
      "Train Epoch: 10 \t Validation loss: 1.189882\n",
      "Train Epoch: 10 \t Validation WER: 1.000000\n",
      "Train Epoch: 10 \t Validation CER: 0.433984\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_and_validation(learning_rate=5e-4, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model = train_from_checkpoint(learning_rate=3e-5, batch_size=16, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, batch_size=4):\n",
    "    \n",
    "    test_label_path = './working/norm_train.csv'\n",
    "    test_audio_directory = './input/bengaliai-speech/test_mp3s'\n",
    "    eval_dataset = BengaliAIDataset(test_label_path, test_audio_directory, data_type='eval')\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    hparams = {\n",
    "    \"n_cnn_layers\": 3,\n",
    "    \"n_rnn_layers\": 5,\n",
    "    \"rnn_dim\": 512,\n",
    "    \"n_class\": len(unique_chars),\n",
    "    \"n_feats\": 128,\n",
    "    \"stride\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": 1\n",
    "    }\n",
    "    \n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "    eval_loader = DataLoader(dataset=eval_dataset,\n",
    "                              batch_size=hparams['batch_size'],\n",
    "                              shuffle=False,\n",
    "                              collate_fn=lambda x: data_processing(x, 'eval'),\n",
    "                              **kwargs)\n",
    "    \n",
    "    iter_meter = IterMeter()\n",
    "    eval_results = evaluate(model, device, eval_loader, iter_meter)\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_results = evaluate_model(trained_model, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnorm = Normalizer()\n",
    "def create_submission_file(decoded_results):\n",
    "    evaluated_sentences = []\n",
    "    file_ids = []\n",
    "    filenames = os.listdir('./input/bengaliai-speech/test_mp3s')\n",
    "    \n",
    "    i=0\n",
    "    for utterance in decoded_results:\n",
    "        # Normalize sentences with bnunicodenormalizer\n",
    "        norm_tokens = [bnorm(char)['normalized'] for char in utterance]\n",
    "        norm_sentence = \"\".join([word for word in norm_tokens if word is not None])\n",
    "        \n",
    "        # Add dari if not present\n",
    "        if norm_sentence[-1] != \"।\":\n",
    "            norm_sentence = norm_sentence + '।'\n",
    "        \n",
    "        # Combine decoded results into sentence\n",
    "        evaluated_sentences.append(''.join(norm_sentence))\n",
    "        \n",
    "        # Organize file IDs\n",
    "        file_ids.append(filenames[i].split('.')[0])\n",
    "    \n",
    "    # Create submission dataframe\n",
    "    sub_df = pd.DataFrame(list(zip(file_ids, evaluated_sentences)),\n",
    "                         columns=['id', 'sentence'])\n",
    "    \n",
    "    # Write submission file\n",
    "#     sub_df.to_csv('/kaggle/working/submission.csv')\n",
    "    \n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_submission_file(decoded_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dawson\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hparams = {\n",
    "    \"n_cnn_layers\": 8,\n",
    "    \"n_rnn_layers\": 4,\n",
    "    \"rnn_dim\": 512,\n",
    "    \"n_class\": len(unique_chars),\n",
    "    \"n_feats\": 128,\n",
    "    \"stride\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 2\n",
    "}\n",
    "\n",
    "train_audio_directory = './input/bengaliai-speech/train_mp3s/'\n",
    "train_label_path = './working/norm_train.csv'\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(BengaliAIDataset(train_label_path, train_audio_directory), \n",
    "                                                             [round(len(BengaliAIDataset(train_label_path, train_audio_directory))*0.999),\n",
    "                                                              round(len(BengaliAIDataset(train_label_path, train_audio_directory))*0.001)])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,  # Change to True for full training runs\n",
    "                          collate_fn=lambda x: data_processing(x, 'training'))\n",
    "\n",
    "model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        )\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                          steps_per_epoch=int(len(train_loader)),\n",
    "                                          epochs=hparams['epochs'],\n",
    "                                          anneal_strategy='cos',\n",
    "                                          div_factor=10,\n",
    "                                          final_div_factor=100,\n",
    "                                          pct_start=0.1)\n",
    "\n",
    "lrs = []\n",
    "steps = []\n",
    "k=0\n",
    "for j in range(hparams['epochs']):\n",
    "    for i in range(len(train_loader)):\n",
    "        lrs.append(scheduler.get_last_lr())\n",
    "        scheduler.step()\n",
    "        steps.append(k)\n",
    "        k+=1\n",
    "lrs = np.stack(lrs, axis=0)\n",
    "steps = np.stack(steps)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(steps, lrs)\n",
    "# plt.yscale('log')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00000000e-05],\n",
       "       [5.01201474e-05],\n",
       "       [5.02402948e-05],\n",
       "       ...,\n",
       "       [1.11416674e-06],\n",
       "       [1.05708337e-06],\n",
       "       [1.00000000e-06]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
